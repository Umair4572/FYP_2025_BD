{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Model Comparison\n",
    "\n",
    "This notebook compares all trained models and identifies the best performer based on:\n",
    "1. **TPR (True Positive Rate / Recall)** - How many actual defaults we catch\n",
    "2. **FPR (False Positive Rate)** - How many good loans we incorrectly flag as risky\n",
    "3. **Other metrics** - AUC-ROC, Precision, F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model results...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root / 'credit_risk_fyp'))\n",
    "\n",
    "from src.config import RESULTS_DIR\n",
    "\n",
    "print(\"Loading model results...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Logistic Regression loaded\n",
      "‚úì Random Forest loaded\n",
      "‚úó XGBoost not found - please train this model first\n",
      "‚úì Neural Network loaded\n",
      "‚úì Stacking Ensemble loaded\n",
      "‚úì Weighted Ensemble loaded\n",
      "\n",
      "================================================================================\n",
      "Successfully loaded 5 models\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Function to safely load model results\n",
    "def load_model_results(model_name, file_name):\n",
    "    try:\n",
    "        with open(RESULTS_DIR / file_name, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"‚úì {model_name} loaded\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚úó {model_name} not found - please train this model first\")\n",
    "        return None\n",
    "\n",
    "# Load all models (base + ensemble)\n",
    "models = {\n",
    "    'Logistic Regression': load_model_results('Logistic Regression', 'logistic_regression_metrics.pkl'),\n",
    "    'Random Forest': load_model_results('Random Forest', 'random_forest_metrics.pkl'),\n",
    "    'XGBoost': load_model_results('XGBoost', 'xgboost_metrics.pkl'),\n",
    "    'Neural Network': load_model_results('Neural Network', 'neural_network_metrics.pkl'),\n",
    "    'Stacking Ensemble': load_model_results('Stacking Ensemble', 'stacking_ensemble_metrics.pkl'),\n",
    "    'Weighted Ensemble': load_model_results('Weighted Ensemble', 'weighted_ensemble_metrics.pkl')\n",
    "}\n",
    "\n",
    "# Filter out models that weren't loaded\n",
    "models = {k: v for k, v in models.items() if v is not None}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Successfully loaded {len(models)} models\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLETE MODEL COMPARISON (ALL 6 MODELS)\n",
      "================================================================================\n",
      "              Model  AUC-ROC  Precision  Recall (TPR)  F1-Score      FPR  Threshold\n",
      "           Baseline 0.708600   0.342500      0.522300  0.413800 0.200000   0.500000\n",
      "Logistic Regression 0.636047   0.264237      0.643219  0.374591 0.445828   0.270530\n",
      "      Random Forest 0.710572   0.344460      0.559179  0.426309 0.264897   0.414141\n",
      "     Neural Network 0.720545   0.343169      0.606292  0.438271 0.288863   0.426963\n",
      "  Stacking Ensemble 0.718666   0.319555      0.670664  0.432862 0.355481   0.212588\n",
      "  Weighted Ensemble 0.721113   0.322776      0.673423  0.436388 0.351709   0.355688\n",
      "\n",
      "‚úì Saved to: c:\\Users\\Faheem\\Desktop\\Umair FYP\\FYP2025\\credit_risk_fyp\\results\\final_model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Baseline results (from original analysis)\n",
    "baseline = {\n",
    "    'AUC-ROC': 0.7086,\n",
    "    'Precision': 0.3425,\n",
    "    'Recall (TPR)': 0.5223,\n",
    "    'F1-Score': 0.4138,\n",
    "    'FPR': 0.2000,\n",
    "    'Threshold': 0.5000\n",
    "}\n",
    "\n",
    "# Build comparison DataFrame\n",
    "rows = []\n",
    "\n",
    "# Add baseline\n",
    "rows.append({\n",
    "    'Model': 'Baseline',\n",
    "    **baseline\n",
    "})\n",
    "\n",
    "# Add trained models\n",
    "for model_name, data in models.items():\n",
    "    tm = data['test_metrics']\n",
    "    \n",
    "    # Handle different metric key names (base models vs ensemble models)\n",
    "    auc = tm.get('roc_auc', tm.get('auc_roc', 0))\n",
    "    precision = tm.get('precision', 0)\n",
    "    recall = tm.get('recall', 0)\n",
    "    f1 = tm.get('f1_score', 0)\n",
    "    fpr = tm.get('false_positive_rate', tm.get('fpr', 0))\n",
    "    \n",
    "    rows.append({\n",
    "        'Model': model_name,\n",
    "        'AUC-ROC': auc,\n",
    "        'Precision': precision,\n",
    "        'Recall (TPR)': recall,\n",
    "        'F1-Score': f1,\n",
    "        'FPR': fpr,\n",
    "        'Threshold': data.get('optimal_threshold', 0.5)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE MODEL COMPARISON (ALL 6 MODELS)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = RESULTS_DIR / 'final_model_comparison.csv'\n",
    "comparison_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úì Saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Identify Best Models by Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BEST MODELS BY METRIC\n",
      "================================================================================\n",
      "\n",
      "üéØ BEST TPR (Catch Most Defaults):\n",
      "   Model: Weighted Ensemble\n",
      "   TPR: 0.6734 (67.34%)\n",
      "   ‚Üí Out of 100 defaults, catches 67.3\n",
      "\n",
      "üéØ BEST FPR (Fewest False Alarms):\n",
      "   Model: Baseline\n",
      "   FPR: 0.2000 (20.00%)\n",
      "   ‚Üí Out of 100 good loans, incorrectly flags only 20.0\n",
      "\n",
      "üéØ BEST AUC-ROC (Overall Performance):\n",
      "   Model: Weighted Ensemble\n",
      "   AUC-ROC: 0.7211\n",
      "\n",
      "üéØ BEST F1-Score (Balanced Performance):\n",
      "   Model: Neural Network\n",
      "   F1-Score: 0.4383\n",
      "\n",
      "üéØ BEST Precision (Most Accurate Predictions):\n",
      "   Model: Random Forest\n",
      "   Precision: 0.3445\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODELS BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# TPR: Higher is better (catch more defaults)\n",
    "best_tpr_idx = comparison_df['Recall (TPR)'].idxmax()\n",
    "print(f\"\\nüéØ BEST TPR (Catch Most Defaults):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_tpr_idx, 'Model']}\")\n",
    "print(f\"   TPR: {comparison_df.loc[best_tpr_idx, 'Recall (TPR)']:.4f} ({comparison_df.loc[best_tpr_idx, 'Recall (TPR)']*100:.2f}%)\")\n",
    "print(f\"   ‚Üí Out of 100 defaults, catches {comparison_df.loc[best_tpr_idx, 'Recall (TPR)']*100:.1f}\")\n",
    "\n",
    "# FPR: Lower is better (fewer false alarms)\n",
    "best_fpr_idx = comparison_df['FPR'].idxmin()\n",
    "print(f\"\\nüéØ BEST FPR (Fewest False Alarms):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_fpr_idx, 'Model']}\")\n",
    "print(f\"   FPR: {comparison_df.loc[best_fpr_idx, 'FPR']:.4f} ({comparison_df.loc[best_fpr_idx, 'FPR']*100:.2f}%)\")\n",
    "print(f\"   ‚Üí Out of 100 good loans, incorrectly flags only {comparison_df.loc[best_fpr_idx, 'FPR']*100:.1f}\")\n",
    "\n",
    "# AUC-ROC: Higher is better (overall discrimination)\n",
    "best_auc_idx = comparison_df['AUC-ROC'].idxmax()\n",
    "print(f\"\\nüéØ BEST AUC-ROC (Overall Performance):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_auc_idx, 'Model']}\")\n",
    "print(f\"   AUC-ROC: {comparison_df.loc[best_auc_idx, 'AUC-ROC']:.4f}\")\n",
    "\n",
    "# F1-Score: Higher is better (balanced precision/recall)\n",
    "best_f1_idx = comparison_df['F1-Score'].idxmax()\n",
    "print(f\"\\nüéØ BEST F1-Score (Balanced Performance):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_f1_idx, 'Model']}\")\n",
    "print(f\"   F1-Score: {comparison_df.loc[best_f1_idx, 'F1-Score']:.4f}\")\n",
    "\n",
    "# Precision: Higher is better (fewer false positives among predictions)\n",
    "best_precision_idx = comparison_df['Precision'].idxmax()\n",
    "print(f\"\\nüéØ BEST Precision (Most Accurate Predictions):\")\n",
    "print(f\"   Model: {comparison_df.loc[best_precision_idx, 'Model']}\")\n",
    "print(f\"   Precision: {comparison_df.loc[best_precision_idx, 'Precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: TPR vs FPR Trade-off Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TPR vs FPR TRADE-OFF ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Goal: Maximize TPR (catch defaults) while minimizing FPR (false alarms)\n",
      "\n",
      "Rank   Model                     TPR        FPR        TPR-FPR Score  \n",
      "--------------------------------------------------------------------------------\n",
      "1      Baseline                  0.5223     0.2000     0.3223         \n",
      "2      Weighted Ensemble         0.6734     0.3517     0.3217         \n",
      "3      Neural Network            0.6063     0.2889     0.3174         \n",
      "4      Stacking Ensemble         0.6707     0.3555     0.3152         \n",
      "5      Random Forest             0.5592     0.2649     0.2943         \n",
      "6      Logistic Regression       0.6432     0.4458     0.1974         \n",
      "\n",
      "üèÜ BEST OVERALL (TPR-FPR Balance): Baseline\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TPR vs FPR TRADE-OFF ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGoal: Maximize TPR (catch defaults) while minimizing FPR (false alarms)\\n\")\n",
    "\n",
    "# Calculate a combined score: TPR - FPR (higher is better)\n",
    "comparison_df['TPR-FPR Score'] = comparison_df['Recall (TPR)'] - comparison_df['FPR']\n",
    "\n",
    "# Sort by this score\n",
    "ranked = comparison_df.sort_values('TPR-FPR Score', ascending=False)\n",
    "\n",
    "print(f\"{'Rank':<6} {'Model':<25} {'TPR':<10} {'FPR':<10} {'TPR-FPR Score':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for idx, (i, row) in enumerate(ranked.iterrows(), 1):\n",
    "    print(f\"{idx:<6} {row['Model']:<25} {row['Recall (TPR)']:<10.4f} {row['FPR']:<10.4f} {row['TPR-FPR Score']:<15.4f}\")\n",
    "\n",
    "best_tradeoff_model = ranked.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ BEST OVERALL (TPR-FPR Balance): {best_tradeoff_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY:\n",
      "   ‚Ä¢ Models evaluated: 6\n",
      "   ‚Ä¢ Best overall (AUC-ROC): Weighted Ensemble\n",
      "   ‚Ä¢ Best TPR-FPR balance: Baseline\n",
      "\n",
      "üí° USE CASE RECOMMENDATIONS:\n",
      "\n",
      "   1Ô∏è‚É£  For CATCHING MOST DEFAULTS (High TPR):\n",
      "       ‚Üí Use: Weighted Ensemble\n",
      "       ‚Üí TPR: 67.34% | FPR: 35.17%\n",
      "\n",
      "   2Ô∏è‚É£  For MINIMIZING FALSE ALARMS (Low FPR):\n",
      "       ‚Üí Use: Baseline\n",
      "       ‚Üí FPR: 20.00% | TPR: 52.23%\n",
      "\n",
      "   3Ô∏è‚É£  For BALANCED PERFORMANCE (Best F1):\n",
      "       ‚Üí Use: Neural Network\n",
      "       ‚Üí F1: 0.4383 | AUC-ROC: 0.7205\n",
      "\n",
      "   4Ô∏è‚É£  For OVERALL BEST (TPR-FPR Balance):\n",
      "       ‚Üí Use: Baseline\n",
      "       ‚Üí TPR-FPR Score: 0.3223\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ANALYSIS COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Models evaluated: {len(comparison_df)}\")\n",
    "print(f\"   ‚Ä¢ Best overall (AUC-ROC): {comparison_df.loc[best_auc_idx, 'Model']}\")\n",
    "print(f\"   ‚Ä¢ Best TPR-FPR balance: {best_tradeoff_model}\")\n",
    "\n",
    "print(\"\\nüí° USE CASE RECOMMENDATIONS:\")\n",
    "print(f\"\\n   1Ô∏è‚É£  For CATCHING MOST DEFAULTS (High TPR):\")\n",
    "print(f\"       ‚Üí Use: {comparison_df.loc[best_tpr_idx, 'Model']}\")\n",
    "print(f\"       ‚Üí TPR: {comparison_df.loc[best_tpr_idx, 'Recall (TPR)']:.2%} | FPR: {comparison_df.loc[best_tpr_idx, 'FPR']:.2%}\")\n",
    "\n",
    "print(f\"\\n   2Ô∏è‚É£  For MINIMIZING FALSE ALARMS (Low FPR):\")\n",
    "print(f\"       ‚Üí Use: {comparison_df.loc[best_fpr_idx, 'Model']}\")\n",
    "print(f\"       ‚Üí FPR: {comparison_df.loc[best_fpr_idx, 'FPR']:.2%} | TPR: {comparison_df.loc[best_fpr_idx, 'Recall (TPR)']:.2%}\")\n",
    "\n",
    "print(f\"\\n   3Ô∏è‚É£  For BALANCED PERFORMANCE (Best F1):\")\n",
    "print(f\"       ‚Üí Use: {comparison_df.loc[best_f1_idx, 'Model']}\")\n",
    "print(f\"       ‚Üí F1: {comparison_df.loc[best_f1_idx, 'F1-Score']:.4f} | AUC-ROC: {comparison_df.loc[best_f1_idx, 'AUC-ROC']:.4f}\")\n",
    "\n",
    "print(f\"\\n   4Ô∏è‚É£  For OVERALL BEST (TPR-FPR Balance):\")\n",
    "print(f\"       ‚Üí Use: {best_tradeoff_model}\")\n",
    "best_idx = ranked.index[0]\n",
    "print(f\"       ‚Üí TPR-FPR Score: {comparison_df.loc[best_idx, 'TPR-FPR Score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
